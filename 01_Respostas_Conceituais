QUESTÕES

1-- Qual o objetivo do comando cache em Spark?

Sabe-se que a maior parte das operações numa RDD são lazy, neste sentido o comando cache melhora a eficiência do código  neste caso, pois permite que os resultados intermediários dessas operações lazy possam ser armazenados e reutilizados repetidamente.


2-- O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?

Em MapReduce, os dados de saída da execução de cada etapa devem ser armazenados no arquivo de arquivo distribuído antes da próxima etapa começar. Essa abordagem tende a ser lenta devido à replicação e ao armazenamento em disco. Se você quiser configurar algo mais complexo, terá que vincular uma série de tarefas do MapReduce e executá-las sequencialmente, cada uma delas com alta latência e ninguém poderá iniciar antes que a anterior tenha completamente terminou.

Ao passo que o Spark permite o desenvolvimento de pipelines complexos de processamento de dados em várias etapas usando gráficos orientados acíclicos (DAG). O Spark permite compartilhar dados na memória entre gráficos, para que vários trabalhos possam funcionar no mesmo conjunto de dados

3-- Qual é a função do SparkContext ?

Atravès do SparkContext passam as configurações que serão utilizadas na alocação de recursos, como memória e processadores, pelos executors. Ele serve tambem para criar RDDs, para colocar Jobs em execução, para criar variáveis de broadcast e acumuladores.
Assim sendo, o SparkContext pode ser considerado como um cliente do ambiente de execução Spark.

4-- Explique com suas palavras o que é Resilient Distributed Datasets (RDD)?

RDD pode ser considerada como a unidade fundamental de dados no Spark e uma das principais características dela 
é a propriedade de ser imutável. 
Após sua criação, não se pode modificá-la, apenas criar um nova RDD baseada nesta RDD. RDD
significa resilient distribute dataset. A sua resiliência vem da tolerância a falhas ou seja se os dados na memória 
forem perdidos, podem ser recriados. O distributed, como o próprio nome já diz, significa que
os dados são armazenados na memória de todo o cluster. Por fim o dataset indica que os dados iniciais podem vir 
de um arquivo ou ser criado programaticamente. Assim sendo, RDD’s são criadas por meio de arquivos, de
dados na memória ou de outras RDD’s. A maioria das aplicações em Spark consistem em manipular RDD’s.

5-- GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?

Sintaxe - groupByKey:
sparkContext.textFile("hdfs://")
                    .flatMap(line => line.split(" ") )
                    .map(word => (word,1))
                    .groupByKey()
                    .map((x,y) => (x,sum(y)) )
                    
sintaxe -- reduceByKey:
sparkContext.textFile("hdfs://")
                    .flatMap(line => line.split(" "))
                    .map(word => (word,1))
                    .reduceByKey((x,y)=> (x+y))

O groupByKey pode causar problemas de disco quando os dados são enviados pela rede e coletados nos executores de redução por não realizar o cálculo de resultados parciais, neste sentido, um volume muito maior de dados é desnecessariamente transferido através dos executores.

Ao passo que em reduceByKey, os dados são combinados em cada partição, apenas uma saída para uma chave em cada partição para enviar pela rede; ou seja; reduceByKey requer combinar todos os seus valores em outro valor exatamente com o mesmo tipo e  para obter um resultado parcial antes de passar esses dados para os executores que vão calcular o resultado final, resultando em um conjunto menor de dados sendo transferido.


B-- ** Explique o que o código Scala abaixo faz **

1. val textFile = sc . textFile ( "hdfs://..." )
2. val counts = textFile . flatMap ( line => line . split ( " " ))
3.           . map ( word => ( word , 1 ))
4.           . reduceByKey ( _ + _ )
5. counts . saveAsTextFile ( "hdfs://..." )

O presente script lê um arquivo-texto (em L1). Depois, cada linha é "transformada/quebrada" em uma sequência de palavras e as sequencias correspondentes a cada linha são transformadas em uma única coleção de palavras (em L2). Cada palavra é então transformada em um mapeamente de [String, Inteiro] ou seja chave-valor, com String igual à própria palavra e Inteiro 1 (em L3) e agregando esses valores por chave usando a operação de soma (em L4). Por finish, a contagem de cada palavra  é realizada pelo RDD e é salvo em um arquivo texto (L5).
